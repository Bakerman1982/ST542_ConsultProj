---
title: "Bridging the Gap:"
subtitle: "Comparing Employer and Educator Expectations in Small Animal Dentistry"
author: Brock Akerman, Hanan Ali, Taylor Cesarski
date: "June, 25, 2025"
format:
  pdf:
    pdf-engine: xelatex
    mainfont: "Times New Roman"
    toc: true
    toc-depth: 2
    number-sections: true
    fig-align: center
    fig-cap-location: top
    fig-pos: 'H'
    geometry: margin=1in
    fontsize: 11pt
    keep-tex: true
    linkcolor: blue
---

```{r package_load, warning=FALSE, include=FALSE}
library(ggplot2)
library(tidyverse)
library(janitor)
library(gtools)
library(naniar)
```

```{r Employer Data load, echo=FALSE, message=FALSE, warning=FALSE, results="hide",include=FALSE, error=FALSE}
#############################################
## Connect, Import, group and primary key. ##
#############################################

#Read data into R environment. 
Employer_Data <- read_csv("Employer_Dentistry_Survey.csv") %>% clean_names()
#Name new object to work with leaving original intact/untouched
Employer_Data_Clean <- Employer_Data

#Add grouping identifier now so if we combine data later, we have our groups delineated. 
Employer_Data_Clean <- Employer_Data_Clean %>%
  mutate(
    group = "Employer", #Categorizes all responses as Employer
    respondent_id = row_number() #Maintains respondent ID
  )


############################
#####  DATA WRANGLING  #####
############################

#Adjust headers for readability and add grouping identifier.
#1.) Remove redundancy in column names, 
#2.) Replace "quid" with "Q", and
#3.) All "_text" with "T".
colnames(Employer_Data_Clean) <- gsub("_text$", "_T", gsub("q", "Q", colnames(Employer_Data_Clean)))
#4.) Format the column headers so that it is sort-able both by question and sub question. So where 
#questions are formatted with header Q10_2, the header will now be Q10_02.  This preserves ordering
#and makes indexing easier. 
emp_pad_question_ids <- function(col_names) {
  col_names %>%
    str_replace_all("Q(\\d{1})(?!\\d)", "Q0\\1") %>%
    str_replace_all("Q(\\d{2})_(\\d{1})(?!\\d)", "Q\\1_0\\2")}

#Apply above formatting changes to data frame column names
colnames(Employer_Data_Clean) <- emp_pad_question_ids(colnames(Employer_Data_Clean))

#Removes Qualtric metadata from the first two rows
Employer_Data_Clean <- Employer_Data_Clean %>% slice(-1, -2)



### QUESTION-BY-QUESTION Cleaning

#------------
#Question #04
#------------
#Renames the text response to index 13 and places it in its order. 
Employer_Data_Clean <- Employer_Data_Clean %>%
  rename(Q04_13 = Q04_12_T)

#------------
#Question #09 does not need correcting. 
#------------

#------------
#Question #16
#------------
#Renames the text response to index 11 and places it in its order. 
Employer_Data_Clean <- Employer_Data_Clean %>%
  rename(Q16_11 = Q16_10_T) 

#Process Q12: Split responses and pivot to wide format
Employer_Data_Clean <- Employer_Data_Clean %>%
  mutate(respondent_id = row_number()) %>%
  separate_rows(Q16, sep = ",") %>%
  mutate(Q16 = str_trim(Q16)) %>%
  #filter(!is.na(Q16)) %>%  # üîß prevent Q16_NA column in pivot_wider
  mutate(Q16 = str_pad(Q16, width = 2, pad = "0"),
         value = 1) %>%
  pivot_wider(
    names_from = Q16,
    names_prefix = "Q16_",
    values_from = value,
    values_fill = 0
  ) %>%
  right_join(select(Employer_Data_Clean, respondent_id), by = "respondent_id")  # üîÅ bring back NA responders


if (!"Q16_09" %in% colnames(Employer_Data_Clean)) {
  Employer_Data_Clean$Q16_09 <- 0
}


#------------
#Question #17
#------------
#Renames the text response to index 14 and places it in its order.
Employer_Data_Clean <- Employer_Data_Clean %>%
  rename(Q17_15 = Q17_14_T)

#------------
#Question #20
#------------
# Create binary indicator columns from Q16.  Recall that Q16 was a multiple responsive
# "Select all that Apply" question where participants could choose however many they 
# wanted.  In the Qualtrics output, these responses were comma-separated.  This query
# breaks each reponse out over a column and assigns values of "0" or "1" depending on 
# whether they choose that particular response. The same issue is address in Question #20. 
Q20_binary <- Employer_Data_Clean %>%
  select(respondent_id, Q20) %>%
  separate_rows(Q20, sep = ",") %>%
  mutate(Q20 = str_trim(Q20)) %>%
  #filter(!is.na(Q20)) %>%   # üëà Prevents Q20_NA from being created
  mutate(Q20 = str_pad(Q20, width = 2, pad = "0"),
         value = 1) %>%
  pivot_wider(
    names_from = Q20,
    names_prefix = "Q20_",
    values_from = value,
    values_fill = 0
  ) %>%
  right_join(select(Employer_Data_Clean, respondent_id), by = "respondent_id")

# Remove the old Q16 columns and join the binary ones on respondent_id
Employer_Data_Clean <- Employer_Data_Clean %>%
  select(-Q20) %>%
  left_join(Q20_binary, by = "respondent_id")

# Renames the text response to index 07 and places it in its order.
Employer_Data_Clean <- Employer_Data_Clean %>%
  rename(Q20_07 = Q20_06_T) %>%
  relocate(Q20_07, .after = Q20_06)

# Add Q16_05 as a column of zeros if it doesn't exist (it does not)
if (!"Q20_05" %in% colnames(Employer_Data_Clean)) {
  Employer_Data_Clean$Q20_05 <- 0
}

#------------
#Question #21
#------------
#Renames the text response to index 07 and places it in its order.
Employer_Data_Clean <- Employer_Data_Clean %>%
  rename(Q21_07 = Q21_06_T) 

#------------
#Question #22 does not need correcting. 
#------------

#------------
#Question #24 does not need correcting. 
#------------

#------------
#Question #25
#------------
# Create binary indicator columns from Q25.  Recall that Q25 was a multiple responsive
# "Select all that Apply" question where participants could choose however many they 
# wanted.  In the Qualtrics output, these responses were comma-separated.  This query
# breaks each reponse out over a column and assigns values of "0" or "1" depending on 
# whether they choose that particular response. The same issue is address in Question #16. 
Q25_binary <- Employer_Data_Clean %>%
  select(respondent_id, Q25) %>%
  separate_rows(Q25, sep = ",") %>%
  mutate(Q25 = str_trim(Q25)) %>%
  #filter(!is.na(Q25)) %>%   # üëà Prevents Q25_NA
  mutate(Q25 = str_pad(Q25, width = 2, pad = "0"),
         value = 1) %>%
  pivot_wider(
    names_from = Q25,
    names_prefix = "Q25_",
    values_from = value,
    values_fill = 0
  ) %>%
  right_join(select(Employer_Data_Clean, respondent_id), by = "respondent_id")


# Remove the old Q16 columns and join the binary ones in
Employer_Data_Clean <- Employer_Data_Clean %>%
  select(-Q25) %>%
  left_join(Q25_binary, by = "respondent_id")

# Add Q25_13 as a column of zeros if it doesn't exist (it does not)
if (!"Q25_13" %in% colnames(Employer_Data_Clean)) {
  Employer_Data_Clean$Q25_13 <- 0
}

# Rename and relocate the "Other" text entry for Q16
Employer_Data_Clean <- Employer_Data_Clean %>%
  rename(Q25_14 = Q25_13_T) %>%
  relocate(Q25_14, .after = Q25_13)

#------------
#Question #26
#------------
#Renames the text response to index 14 and places it in its order. 
Employer_Data_Clean <- Employer_Data_Clean %>%
  rename(Q26_14 = Q26_13_T) 


##########################
##### FILTER DATASET #####
##########################
#"Q20_NA"
#"Q25_NA"


#Remove irrelevant/problematic columns to the research/stats questions
Employer_Data_Clean <- Employer_Data_Clean %>%
  mutate(progress = as.numeric(progress)) %>%
  filter(Q02 == '1', progress >= 50) %>%
  select(
    ip_address, progress, duration_in_seconds,
    finished, recorded_date, response_id, group, respondent_id,
    starts_with("Q04"),
    starts_with("Q09"),
    starts_with("Q16"),
    starts_with("Q17"),
    starts_with("Q20"),
    starts_with("Q21"),
    starts_with("Q22"),
    starts_with("Q24"),
    starts_with("Q25"),
    starts_with("Q26"),
    -Q16_NA,  # üëà drop this
    -Q20_NA,  # üëà drop this
    -Q25_NA
  )

#Identify metadata columns to keep at the front
emp_meta_cols <- c("ip_address", "progress", "duration_in_seconds",
               "finished", "recorded_date", "response_id", "group", "respondent_id")

#Identify and sort Q columns using mixedsort
emp_q_cols <- setdiff(colnames(Employer_Data_Clean), emp_meta_cols)
emp_q_cols_sorted <- mixedsort(emp_q_cols)

#Reorder the data frame
Employer_Data_Clean <- Employer_Data_Clean[, c(emp_meta_cols, emp_q_cols_sorted)]
```

```{r Educator Data load, echo=FALSE, message=FALSE, warning=FALSE, results="hide",include=FALSE, error=FALSE}
#############################################
## Connect, Import, group and primary key. ##
#############################################

#Read data into R environment. 
Educator_Data <- read_csv("PCVE_Dentistry_Survey.csv") %>% clean_names()
#Name new object to work with leaving original intact/untouched
Educator_Data_Clean <- Educator_Data

#Add identifier affirming educator responses. 
Educator_Data_Clean <- Educator_Data_Clean %>%
  mutate(
    group = "Educator",
    respondent_id = row_number()
  )

############################
#####  DATA WRANGLING  #####
############################

#Adjust headers for readability and add grouping identifier.
  #1.) Remove redundancy in column names, 
  #2.) Replace "quid" with "Q", and
  #3.) All "_text" with "T".
colnames(Educator_Data_Clean) <- gsub("_text$", "_T", gsub("q", "Q", colnames(Educator_Data_Clean)))
  #4.) Format the column headers so that it is sort-able both by question and sub question. So where 
    #questions are formatted with header Q10_2, the header will now be Q10_02.  This preserves ordering
    #and makes indexing easier. 
edu_pad_question_ids <- function(col_names) {
  col_names %>%
    str_replace_all("Q(\\d{1})(?!\\d)", "Q0\\1") %>%
    str_replace_all("Q(\\d{2})_(\\d{1})(?!\\d)", "Q\\1_0\\2")}

#Apply above formatting changes to data frame column names
colnames(Educator_Data_Clean) <- edu_pad_question_ids(colnames(Educator_Data_Clean))

#Removes Qualtric metadata from the first two rows
Educator_Data_Clean <- Educator_Data_Clean %>% slice(-1, -2)


### QUESTION-BY-QUESTION Cleaning

#------------
#Question #04
#------------
#Renames the text response to index 13 and places it in its order. 
Educator_Data_Clean <- Educator_Data_Clean %>%
  rename(Q04_13 = Q04_12_T)

#------------
#Question #07 does not need correcting. 
#------------

#------------
#Question #12
#------------
  #Renames the text response to index 11 and places it in its order. 
Educator_Data_Clean <- Educator_Data_Clean %>%
  rename(Q12_11 = Q12_10_T) 

  #Process Q12: Split responses and pivot to wide format
Educator_Data_Clean <- Educator_Data_Clean %>%
  # Create a temporary respondent ID if not already done
  mutate(respondent_id = row_number()) %>%
  # Separate comma-separated values into long format
  separate_rows(Q12, sep = ",") %>%
  # Trim whitespace if any
  mutate(Q12 = str_trim(Q12)) %>%
  # Pad values with leading zero for consistent column naming (01, 02, ..., 11)
  mutate(Q12 = str_pad(Q12, width = 2, pad = "0"),
         value = 1) %>%
  # Pivot wider to get binary indicator columns
  pivot_wider(
    names_from = Q12,
    names_prefix = "Q12_",
    values_from = value,
    values_fill = 0
  )

#------------
#Question #13
#------------
#Renames the text response to index 14 and places it in its order.
Educator_Data_Clean <- Educator_Data_Clean %>%
  rename(Q13_14 = Q13_13_T)

#------------
#Question #16
#------------
# Create binary indicator columns from Q16.  Recall that Q16 was a multiple responsive
# "Select all that Apply" question where participants could choose however many they 
# wanted.  In the Qualtrics output, these responses were comma-separated.  This query
# breaks each reponse out over a column and assigns values of "0" or "1" depending on 
# whether they choose that particular response. The same issue is address in Question #20. 
Q16_binary <- Educator_Data_Clean %>%
  select(respondent_id, Q16) %>%
  separate_rows(Q16, sep = ",") %>%
  mutate(Q16 = str_trim(Q16)) %>%
  mutate(Q16 = str_pad(Q16, width = 2, pad = "0"),
         value = 1) %>%
  pivot_wider(
    names_from = Q16,
    names_prefix = "Q16_",
    values_from = value,
    values_fill = 0
  )

  # Remove the old Q16 columns and join the binary ones on respondent_id
Educator_Data_Clean <- Educator_Data_Clean %>%
  select(-Q16) %>%
  left_join(Q16_binary, by = "respondent_id")

  # Renames the text response to index 07 and places it in its order.
Educator_Data_Clean <- Educator_Data_Clean %>%
  rename(Q16_07 = Q16_06_T) %>%
  relocate(Q16_07, .after = Q16_06)

  # Add Q16_05 as a column of zeros if it doesn't exist (it does not)
if (!"Q16_05" %in% colnames(Educator_Data_Clean)) {
  Educator_Data_Clean$Q16_05 <- 0
}

#------------
#Question #17
#------------
#Renames the text response to index 07 and places it in its order.
Educator_Data_Clean <- Educator_Data_Clean %>%
  rename(Q17_07 = Q17_06_T) 

#------------
#Question #18 does not need correcting. 
#------------

#------------
#Question #19 does not need correcting. 
#------------

#------------
#Question #20
#------------
# Create binary indicator columns from Q16.  Recall that Q16 was a multiple responsive
# "Select all that Apply" question where participants could choose however many they 
# wanted.  In the Qualtrics output, these responses were comma-separated.  This query
# breaks each reponse out over a column and assigns values of "0" or "1" depending on 
# whether they choose that particular response. The same issue is address in Question #16. 
Q20_binary <- Educator_Data_Clean %>%
  select(respondent_id, Q20) %>%
  separate_rows(Q20, sep = ",") %>%
  mutate(Q20 = str_trim(Q20)) %>%
  mutate(Q20 = str_pad(Q20, width = 2, pad = "0"),
         value = 1) %>%
  pivot_wider(
    names_from = Q20,
    names_prefix = "Q20_",
    values_from = value,
    values_fill = 0
  )

# Remove the old Q16 columns and join the binary ones in
Educator_Data_Clean <- Educator_Data_Clean %>%
  select(-Q20) %>%
  left_join(Q20_binary, by = "respondent_id")

# Rename and relocate the "Other" text entry for Q16
Educator_Data_Clean <- Educator_Data_Clean %>%
  rename(Q20_14 = Q20_13_T) %>%
  relocate(Q20_14, .after = Q20_13)

#------------
#Question #21
#------------
#Renames the text response to index 14 and places it in its order. 
Educator_Data_Clean <- Educator_Data_Clean %>%
  rename(Q21_14 = Q21_13_T) 


##########################
##### FILTER DATASET #####
##########################

#Remove irrelevant/problematic columns to the research/stats questions
Educator_Data_Clean <- Educator_Data_Clean %>%
  mutate(progress = as.numeric(progress)) %>%
  filter(Q44 != "2", progress >= 50) %>%
  select(
    ip_address, progress, duration_in_seconds,
    finished, recorded_date, response_id, group, respondent_id,
    starts_with("Q04"),
    starts_with("Q07"),
    starts_with("Q12"),
    starts_with("Q13"),
    starts_with("Q16"),
    starts_with("Q17"),
    starts_with("Q18"),
    starts_with("Q19"),
    starts_with("Q20"),
    starts_with("Q21")
  )



#Identify metadata columns to keep at the front
edu_meta_cols <- c("ip_address", "progress", "duration_in_seconds",
               "finished", "recorded_date", "response_id", "group", "respondent_id")

#Identify and sort Q columns using mixedsort
edu_q_cols <- setdiff(colnames(Educator_Data_Clean), edu_meta_cols)
edu_q_cols_sorted <- mixedsort(edu_q_cols)

#Reorder the data frame
Educator_Data_Clean <- Educator_Data_Clean[, c(edu_meta_cols, edu_q_cols_sorted)]
```

# Abstract

# Introduction

Dr. Mariea Ross-Estrada, a faculty member at North Carolina State University's College of Veterinary Medicine, is exploring whether there are differences between the expectations of small animal primary care veterinary employers and veterinary educators regarding new graduates' competencies in dentistry. Through her own professional experience and conversations with colleagues, Dr. Ross-Estrada observed that many veterinarians must rely on on-the-job training to gain the skills necessary in small animal dentistry. These shared experiences prompted her to investigate whether there is a misalignment in what is taught in veterinary programs and what is expected in clinical practice. 

To explore this question, Dr. Ross-Estrada distributed two surveys: one to medical directors and private practice owners and the other to primary care veterinary educators. Both surveys included similar questions regarding what early-career veterinarians are expected to have learned during their education and the skills they are expected to perform in practice.

## Research Question

How do small animal primary care employers (medical directors and practice owners) and primary care veterinary educators differ in regards to their expectations of early career veterinary graduates‚Äô competencies in small animal dentistry?

## Statistical Questions

1. Are there significant differences between educators and practice owners in their belief that new graduates are competent in key dental skills on their first day of practice? 

2. Is there a difference between educators and practice owners in their reports (educators‚Äô actual teaching vs. owners‚Äô perceptions) of which dental skills were taught in the pre-clinical DVM curriculum for recent graduates?

3. Is there a difference between educators and practice owners in their level of agreement about whether specific dental skills should be taught pre-clinically?

4. Do employers and educators differ in their expectations about how many dental procedures new graduates should complete during clinical training?

5. Is there difference between the instructional formats in dentistry reported by DVM programs  and the formats perceived by employers to have been completed by early career veterinarians?

6. Do educators and employers differ in their views on which formats of clinical instruction in dentistry should be required for DVM students as part of their clinical training?

7. Is there a difference between the clinical dentistry skills that educators report DVM students are learning during their clinical training and the skills that employers believe recent graduates have completed as part of their DVM program?

8. Do educators and employers differ in their opinions about which clinical dentistry skills DVM students should be required to practice or learn during their clinical training?

# Data

## Data Description

Two separate surveys were administered to mutually exclusive groups: veterinary employers who have worked with students, and educators who have taught students. There was no overlap between these groups and they can be assumed to be independent.

The employer data set consists of responses from 29 participants answering 40 questions, while the educator data set includes 43 participants answering 34 questions. Each group was asked a single qualifying question to determine eligibility for participation, along with nine questions covering demographics and institutional context. Educators were then presented with 24 competency and sentiment-based questions, while employers answered 30 such items focused on professional expectations and training in veterinary medicine.

Survey questions took several forms. Some were binary (Yes/No), particularly those related to demographics and institutional affiliation. Others used a "select all that apply" format, commonly seen in questions asking respondents to identify procedures performed at their practice. Many of these questions were followed by Likert-scale items. The Likert scales were even-numbered and omitted a neutral option, which may have contributed to at least two instances where respondents selected both ‚Äúagree‚Äù and ‚Äúdisagree‚Äù for the same item.

Several questions offered an ‚ÄúOther‚Äù response with a text box for elaboration. A few required numeric input, such as estimates of hours worked or the number of practicing veterinarians. These integer fields were not restricted by any upper bound, regardless of contextual reasonableness.

Survey completion time differed by group. Educators, on average, spent more time completing the survey than employers. While no follow-up question asked participants to explain their response time, this discrepancy may reflect greater engagement or a tendency for more elaborated responses among educators. It may also suggest a greater willingness among educators to participate more thoughtfully. The box plot below illustrates the distribution of survey duration (in minutes) by group.

```{r Data_Desc_01, echo=FALSE, fig.align='center'}
#Average time for survey completion
# Convert and clean both datasets
clean_employer <- Employer_Data[-c(1,2), ] %>%
  mutate(
    duration_in_seconds = as.numeric(duration_in_seconds),
    duration_minutes = duration_in_seconds / 60,
    group = "Employer"
  )

clean_educator <- Educator_Data[-c(1,2), ] %>%
  mutate(
    duration_in_seconds = as.numeric(duration_in_seconds),
    duration_minutes = duration_in_seconds / 60,
    group = "Educator"
  )

# Combine for optional single-geom plotting, or keep separate for layers
ggplot() +
  geom_boxplot(data = clean_employer,
               aes(x = group, y = duration_minutes),
               fill = "steelblue", alpha = 0.6) +
  geom_boxplot(data = clean_educator,
               aes(x = group, y = duration_minutes),
               fill = "darkorange", alpha = 0.6) +
  coord_cartesian(ylim = c(0, 80)) +  # Optional: zoom to 0‚Äì60 minutes
  labs(
    title = "Survey Completion Time by Group",
    x = "Respondent Group",
    y = "Duration (minutes)"
  ) +
  theme_minimal()
```

Regarding the proportion of the survey completed, employer responses were more variable‚Äîspanning the full range from partial to full completion. In contrast, educators tended to complete more of the survey, with a concentration near full completion and a less pronounced left tail. The density plot below visualizes these differences in survey progress across groups.

```{r Data_Desc_02, echo=FALSE, fig.align='center'}
#Average percent of survey completed
combined_progress <- bind_rows(
  Employer_Data[-c(1,2), ] %>% mutate(group = "Employer"),
  Educator_Data[-c(1,2), ] %>% mutate(group = "Educator")
) %>%
  mutate(progress = as.numeric(progress))

ggplot(combined_progress, aes(x = progress, fill = group)) +
  geom_density(alpha = 0.4) +
  labs(title = "Survey Progress Density by Group", x = "Progress (%)", y = "Density") +
  theme_minimal()

```

```{r Data_Desc_03, eval=FALSE, fig.align='center'}
#SPACER FOR MORE ANALYTICS
From the educators dataset we could talk about:
* Q27; How long have you been teaching DVM students in the clinical training portion of a DVM program?
* Q28; Does your institution have a teaching hospital?
* Q29; On average, how many dental procedures does your primary care service perform each week?


```

```{r Data_Desc_04, eval=FALSE, fig.align='center'}
#SPACER FOR MORE ANALYTICS

From the employers dataset we could talk about:
* Q33; Which of the following best describes your job setting or organization?
* Q36; On average, how many dental procedures does your practice/oganization/institution perform each week
* Q37; Which of the following best describes the person who completed this survey?
```

## Data Source

Survey data were collected using Qualtrics, a cloud-based experience management platform commonly used for gathering feedback and sentiment across workforce domains. Participants from the educator survey were recruited via email invitation sent by the researcher, using pre-existing contact lists. Dr. Ross-Estrada distributed the employer survey to her personal and professional networks online. Participation was voluntary and anonymous. There was no incentive offered for completing the survey.

## Preprocessing Description

Although the employer and educator data sets shared a similar structure, they were not identical. Most pre-processing steps were applied uniformly across both data sets, with minor deviations where needed.

The data sets were imported into the RStudio environment (version 2024.04.1 Build 748). A new variable was created to label the data source ("Educator" or "Employer") for later grouping and visualization. The existing respondent_id column served as a unique identifier and was treated as the primary key.

Initial cleaning involved removing extraneous metadata included by Qualtrics‚Äîsuch as survey start and end times, IP addresses, geolocation data, and question display logic‚Äîall of which were irrelevant to the analysis. These columns were trimmed to streamline the dataset for subsequent transformation and statistical work.

Column names in the original Qualtrics export were alphanumeric but often ambiguous and misleading. Many variable names did not match the corresponding survey question numbers. Our team manually mapped the exported column names to their corresponding survey questions and responses by referencing adjacent metadata fields and using deductive reasoning. This process allowed us to build an index-based column naming structure, which greatly improved the manageability and interpretability of the dataset.

Before diving into question-specific analysis, we first identified the subset of survey questions relevant to our research objectives. All unrelated or out-of-scope items were removed. This step reduced the employer dataset from 176 columns to 100, and the educator dataset from 171 columns to 102.

Several formatting inconsistencies also needed to be resolved. Some multi-select questions appeared in the form of comma-separated text responses within a single column, while others were exported into multiple binary columns. Additionally, for certain questions, a response option that received zero selections was dropped entirely by Qualtrics. To standardize these issues, we implemented a script to ‚Äúexplode‚Äù comma-separated responses into individual binary columns. For dropped columns, we manually reintroduced them as zero-filled dummy variables to preserve the full response structure.

Finally, we filtered out participants who answered less than half of the survey. We also excluded:

-   Employers who responded ‚ÄúNo‚Äù to the question: ‚ÄúDo you work with early career veterinarians (someone who has graduated from a DVM program after May 2021)?‚Äù

-   Educators who responded ‚ÄúNo‚Äù to: ‚ÄúDo you teach in any capacity of the dental curriculum at your institution?‚Äù

After all preprocessing steps, the final cleaned datasets consisted of 13 employer participants and 30 educator participants.

# Statistical Methods

## Method Description

Likert scale questions (statistical questions #1,3, 6, and 8): These questions utilize a likert scale to answer Strongly Agree, Agree, Disagree, or Strongly Disagree on a variety of skills and procedures. To analyze this data, we will use diverging stacked bar charts and tables for exploratory data analysis and the Mann-Whitney U Test for the formal hypothesis procedure. The Mann-Whitney U Test is a non-parametric test that does not rely on an assumption of normality. It does require independence between datasets and it is reasonable to assume that the educators and employers were independent from each other when taking the survey. The Mann-Whitney test particularly works well with these questions due to the ordinal (and therefore ranked) nature of the data.

Select all that apply questions (statistical questions #2, 5, and 7): These questions asked the participants to ‚Äúselect all that apply‚Äù as it relates to skills in pre-clinical curriculum, format of dental instruction, and skills for clinical training respectively. We will utilize frequency tables and bar plots to explore the data for these questions and use Fisher‚Äôs Exact Test as a formal inference procedure for the comparison of the two groups. Fisher‚Äôs Exact Test works with categorical data with independent samples, in this case educators and employers. In this context, it is preferred over Chi-Squared Tests due to the small sample size and therefore not meeting the expected count threshold that is required to proceed with Chi-Squared Tests. Given the small sample sizes, we acknowledge the limited power of these analyses and may consider post hoc power analyses for these tests.

Numerical entry questions: (statistical question #4): This question asks participants to enter a number related to the number of dental procedures that should be completed during training in different areas. We plan to produce box plots and/or histograms to visually examine the data. Depending on the normality or lack thereof of the distributions, we will then conduct either a two-sample t-test or a Mann-Whitney U Test. As mentioned above, the Mann Whitney U test is a non-parametric test that does not depend on the assumption of normality. If the assumption on normality is met, we can consider a two-sample t test for this analysis.

# Results

### S1 Are there significant differences between educators and practice owners in their belief that new graduates are competent in key dental skills on their first day of practice?

### S2 Is there a difference between educators and practice owners in their reports (educators‚Äô actual teaching vs. owners‚Äô perceptions) of which dental skills were taught in the pre-clinical DVM curriculum for recent graduates?

### S3 Is there a difference between educators and practice owners in their level of agreement about whether specific dental skills should be taught pre-clinically?

### S4 Do employers and educators differ in their expectations about how many dental procedures new graduates should complete during clinical training?

### S5 Is there difference between the instructional formats in dentistry reported by DVM programs and the formats perceived by employers to have been completed by early career veterinarians?

Both educators and employers were asked a question relating to the format of instruction during the clinical year.

Employers were asked: "What format of clinical instruction in dentistry do you believe that the early career veterinarians (individuals who have graduated from a DVM program after May 2021) hired into your practice/organization/institution completed as part of their DVM training?  Select all that apply."

Educators were asked: "What format of instruction in dentistry does your DVM program provide during the clinical year?  Select all that apply."

Since the question was of the format "select all that apply," participants were able to select more than one response and percentages will not add to 100%. Results reported below are the percentages of their respective group (educators or employers) that selected the given format of instruction. Percentages were utilized due to the difference in sample size.

Educators and employers differed in their perceptions of clinical dental instruction formats completed by early career veterinarians. While a majority in both groups acknowledged didactic instruction, educators reported higher rates of wet lab (73.3% vs. 46.2%) and live patient training (93.3% vs. 38.5%) compared to employers. Conversely, employers more frequently identified simulation training (30.8% vs. 16.7%) than educators. These differences suggest varying expectations or awareness between the two groups regarding dental training experiences of recent graduates.


*Add here about people who entered a text response.

```{r echo=FALSE}
#Add custom labels to final bar graph
option_labels <- c(
  "01" = "Didactic",
  "02" = "Simulation",
  "03" = "Wet Lab",
  "04" = "Live Patient",
  "05" = "None of these"
)

# Count total respondents per group
n_employers <- nrow(Employer_Data_Clean)
n_educators <- nrow(Educator_Data_Clean)

#Employer data: calculate percentage of total dataset
employer_percent <- Employer_Data_Clean %>%
  select(Q20_01:Q20_05) %>%
  summarise(across(everything(), ~ sum(. == 1, na.rm = TRUE))) %>%
  pivot_longer(everything(), names_to = "option", values_to = "count") %>%
  mutate(group = "Employer",
         option = str_replace(option, "Q20_", ""),
         percent = count / n_employers * 100)

#Educator data: calculate percentage of total dataset
educator_percent <- Educator_Data_Clean %>%
  select(Q16_01:Q16_05) %>%
  summarise(across(everything(), ~ sum(. == 1, na.rm = TRUE))) %>%
  pivot_longer(everything(), names_to = "option", values_to = "count") %>%
  mutate(group = "Educator",
         option = str_replace(option, "Q16_", ""),
         percent = count / n_educators * 100)

#Combine
combined_percent <- bind_rows(employer_percent, educator_percent) %>%
  mutate(option = case_when(
    option == "01" ~ "Didactic",
    option == "02" ~ "Simulation",
    option == "03" ~ "Wet Lab",
    option == "04" ~ "Live Patient",
    option == "05" ~ "None of These",
    TRUE ~ option
  ))

#Keep the order of the bars as the order of the question
combined_percent$option <- factor(combined_percent$option,
                                  levels = c("Didactic", "Simulation", "Wet Lab", "Live Patient", "None of These"))

#Plot percentages
ggplot(combined_percent, aes(x = option, y = percent, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title ="Perceived Clinical Instruction Formats in Dentistry\nCompleted by Early Career Veterinarians",
       x = "Format of Instruction",
       y = "Percent of Respondents",
       fill = "Group") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  ylim(0, max(combined_percent$percent) + 10) +
  geom_text(aes(label = paste0(round(percent, 1), "%")), 
          position = position_dodge(width = 0.9), 
          vjust = -0.3, size = 3)
```


The table below shows the percentages of each format selected (as seen in the bar graph above) and the p-value associated with performing Fisher's Exact Test on each format.

Didactic instruction was selected by 53.3% of educators and 61.5% of employers, with no statistically significant difference between the groups (p=0.743). There was also no statistically significant difference in simulation (p=0.417) and wet lab instruction (p=0.162).

On the other hand, live patient instruction was selected by 93.3% of educators, but only 38.5% of employers. This difference was found to be statistically significant with a p-value of 0.0003.

*Don't know whether to talk about "none of the above" being significant.


```{r echo = FALSE}
# Total respondents per group
n_employers <- nrow(Employer_Data_Clean)
n_educators <- nrow(Educator_Data_Clean)

# Function to get counts of selected responses
get_counts <- function(data, vars, n_total) {
  data %>%
    select(all_of(vars)) %>%
    summarise(across(everything(), ~ sum(. == 1, na.rm = TRUE))) %>%
    pivot_longer(everything(), names_to = "option", values_to = "count") %>%
    mutate(percent = count / n_total * 100,
           option = str_replace(option, "Q[0-9]+_", ""))
}

# Get counts for each group
employer_counts <- get_counts(Employer_Data_Clean, paste0("Q20_0", 1:5), n_employers)
educator_counts <- get_counts(Educator_Data_Clean, paste0("Q16_0", 1:5), n_educators)

# Merge counts by option
combined_counts <- educator_counts %>%
  rename(educator_count = count, educator_percent = percent) %>%
  inner_join(
    employer_counts %>% rename(employer_count = count, employer_percent = percent),
    by = "option"
  )

# Map option codes to labels
combined_counts <- combined_counts %>%
  mutate(Format = case_when(
    option == "01" ~ "Didactic",
    option == "02" ~ "Simulation",
    option == "03" ~ "Wet Lab",
    option == "04" ~ "Live Patient",
    option == "05" ~ "None of These"
  ))

# Calculate p-values using Fisher's Exact Test for each option
combined_counts <- combined_counts %>%
  rowwise() %>%
  mutate(
    p_value = {
      a <- educator_count
      b <- n_educators - a
      c <- employer_count
      d <- n_employers - c
      contingency_matrix <- matrix(c(a, b, c, d), nrow = 2, byrow = TRUE)
      fisher.test(contingency_matrix)$p.value
    }
  ) %>%
  ungroup()

# Select and arrange columns for output
final_table <- combined_counts %>%
  select(Format, educator_percent, employer_percent, p_value) %>%
  arrange(match(Format, c("Didactic", "Simulation", "Wet Lab", "Live Patient", "None of These")))

# Format percentages nicely
final_table <- final_table %>%
  mutate(
    educator_percent = round(educator_percent, 1),
    employer_percent = round(employer_percent, 1),
    p_value = signif(p_value, 3)
  ) %>%
  rename(
    `% of Educators Selected` = educator_percent,
    `% of Employers Selected` = employer_percent,
    `P-value` = p_value
  )

library(knitr)
knitr::kable(final_table, col.names = c("Format", "% of Educators", "% of Employers", "P-value"))

```







### S6 Do educators and employers differ in their views on which formats of clinical instruction in dentistry should be required for DVM students as part of their clinical training?

In question #21 of the employers version of the survey, participants were asked, "Which of the following types of [*clinical instruction in dentistry*]{.underline} do you think that DVM students should be required to complete as part of a [*DVM program*]{.underline}? Select one response for each of the instructional types listed below." The analogue of this question for educators was survey question #17. We want to make a note that the educators survey question had a slight variation in phraseology. It asked, "Which of the following types of [*instruction*]{.underline} do you think DVM students should be required to complete as part of their [*clinical training*]{.underline}? Select one response for each type of instruction listed below.".

This question is targeting how participants feel about what dental veterinarian medical programs are teaching and what topics should be required in their curriculum. The research question asked if there was a difference in opinions on this matter. To infer from the data, we will use the Mann-Whitney U-Test, a non-parametric test also known as Wilcoxin Rank Sign Test. This test assumes mutual exclusivity between groups.

```{r S6, , echo=FALSE, warning=FALSE, fig.align='center'}
educator_cols <- paste0("Q17_", sprintf("%02d", 1:7))
employer_cols  <- paste0("Q21_", sprintf("%02d", 1:7))

# Initialize a results list
mw_results <- list()

# For loop that 
for (i in seq_along(educator_cols)) {
  q_edu <- educator_cols[i]
  q_emp <- employer_cols[i]
  
  # Coerce to numeric
  edu_vals <- as.numeric(Educator_Data_Clean[[q_edu]])
  emp_vals <- as.numeric(Employer_Data_Clean[[q_emp]])
  
  # Remove NAs.  Wilcoxin handles NA values automatically. 
  edu_vals <- edu_vals[!is.na(edu_vals)]
  emp_vals <- emp_vals[!is.na(emp_vals)]
  
  # Only run test if both groups have >1 value
  if (length(edu_vals) > 1 && length(emp_vals) > 1) {
    test_result <- wilcox.test(edu_vals, emp_vals, exact = FALSE)
    
    mw_results[[q_edu]] <- list(
      question = q_edu,
      W = test_result$statistic,
      p_value = test_result$p.value
    )
  } else {
    mw_results[[q_edu]] <- list(
      question = q_edu,
      W = NA,
      p_value = NA
    )
  }
}

# This produces a table of 
mw_df <- do.call(rbind, lapply(mw_results, as.data.frame))
mw_df <- as.data.frame(mw_df)
mw_df$p_value <- as.numeric(mw_df$p_value)
mw_df$question <- rownames(mw_df)
mw_df$significance <- cut(mw_df$p_value,
                          breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
                          labels = c("***", "**", "*", ".", "ns"))
print(mw_df)
```

```{r TestSpace8, eval=FALSE, echo=FALSE, fig.align='center'}

sapply(Educator_Data_Clean[ , educator_cols[5:7]], function(x) sum(!is.na(as.numeric(x))))
sapply(Employer_Data_Clean[ , employer_cols[5:7]], function(x) sum(!is.na(as.numeric(x))))

Employer_Data_Clean %>% select(starts_with("Q21"))


# Count NAs for reference
sapply(Educator_Data_Clean[ , educator_cols[5:7]], function(x) sum(!is.na(as.numeric(x))))
sapply(Employer_Data_Clean[ , employer_cols[5:7]], function(x) sum(!is.na(as.numeric(x))))

Employer_Data_Clean %>% select(starts_with("Q21"))

# STEP 1: Create combined_mirrored data
combined_mirrored <- bind_rows(
  Educator_Data_Clean %>%
    select(Q17_01, Q17_02, Q17_03) %>%
    pivot_longer(everything(), names_to = "Question", values_to = "Response") %>%
    mutate(
      Group = "Educator",
      Item = case_when(
        Question == "Q17_01" ~ "Didactic",
        Question == "Q17_02" ~ "Simulation",
        Question == "Q17_03" ~ "Wet Lab"
      )
    ),
  
  Employer_Data_Clean %>%
    select(Q21_01, Q21_02, Q21_03) %>%
    pivot_longer(everything(), names_to = "Question", values_to = "Response") %>%
    mutate(
      Group = "Employer",
      Item = case_when(
        Question == "Q21_01" ~ "Didactic",
        Question == "Q21_02" ~ "Simulation",
        Question == "Q21_03" ~ "Wet Lab"
      )
    )
) %>%
  filter(!is.na(Response)) %>%
  mutate(
    Response = factor(Response, levels = 1:4, 
                      labels = c("Strongly Disagree", "Disagree", "Agree", "Strongly Agree")),
    Count = 1,
    Direction = if_else(Group == "Employer", -1, 1)
  ) %>%
  group_by(Group, Item, Response) %>%
  summarise(n = sum(Count) * unique(Direction), .groups = "drop")

# STEP 2: Grouped labels and colors
combined_mirrored <- combined_mirrored %>%
  mutate(
    Response_Grouped = paste(Response, Group, sep = " - "),
    Response = factor(Response, levels = c("Strongly Disagree", "Disagree", "Agree", "Strongly Agree")),
    Response_Grouped = factor(Response_Grouped, levels = c(
      "Strongly Disagree - Educator", "Disagree - Educator", "Agree - Educator", "Strongly Agree - Educator",
      "Strongly Disagree - Employer", "Disagree - Employer", "Agree - Employer", "Strongly Agree - Employer"
    ))
  )

custom_colors <- c(
  "Strongly Disagree - Educator" = "#d73027",
  "Disagree - Educator"          = "#f46d43",
  "Agree - Educator"             = "#fdae61",
  "Strongly Agree - Educator"    = "#fee090",
  "Strongly Disagree - Employer" = "#4575b4",
  "Disagree - Employer"          = "#74add1",
  "Agree - Employer"             = "#abd9e9",
  "Strongly Agree - Employer"    = "#e0f3f8"
)

# STEP 3: Plot
ggplot(combined_mirrored, aes(x = Item, y = n, fill = Response_Grouped)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_y_continuous(labels = abs) +
  scale_fill_manual(values = custom_colors) +
  labs(
    title = "Back-to-Back Comparison of Educator vs Employer Responses",
    x = "Instructional Method",
    y = "Response Count",
    fill = "Likert Response by Group"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")


library(tidyverse)

educator_cols <- paste0("Q17_", sprintf("%02d", 1:7))
employer_cols <- paste0("Q21_", sprintf("%02d", 1:7))

mw_df <- map2_dfr(educator_cols, employer_cols, function(q_edu, q_emp) {
  edu_vals <- as.numeric(Educator_Data_Clean[[q_edu]])
  emp_vals <- as.numeric(Employer_Data_Clean[[q_emp]])

  # Remove NAs
  edu_vals <- edu_vals[!is.na(edu_vals)]
  emp_vals <- emp_vals[!is.na(emp_vals)]

  if (length(edu_vals) > 1 && length(emp_vals) > 1) {
    test_result <- wilcox.test(edu_vals, emp_vals, exact = FALSE)
    tibble(
      question = q_edu,
      W = test_result$statistic,
      p_value = test_result$p.value
    )
  } else {
    tibble(
      question = q_edu,
      W = NA_real_,
      p_value = NA_real_
    )
  }
})

# Add significance stars
mw_df <- mw_df %>%
  mutate(significance = cut(p_value,
                            breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
                            labels = c("***", "**", "*", ".", "ns")))

print(mw_df)


```

### S7 Is there a difference between the clinical dentistry skills that educators report DVM students are learning during their clinical training and the skills that employers believe recent graduates have completed as part of their DVM program?

Both educators and employers were asked a question related to skills learned and practiced during the clinical year.

On question 25 of their survey, employers were asked: "Which of the following skills do you think that individuals who graduated with a DVM degree after May 2021 completed during the clinical training portion of their DVM program? Select all that apply."

On question 20 of their survey, educators were asked: "Which of the following skills are DVM students at your institution practicing/learning during the clinical training portion of the DVM program? Select all that apply."

Since the question was of the format "select all that apply," participants were able to select more than one response and percentages will not add to 100%. Results reported below are the percentages of their respective group (educators or employers) that selected the given format of instruction. Percentages were utilized due to the difference in sample size.

```{r, echo = FALSE, fig.width=10, fig.height=7}

# Labels for each skill
option_labels <- c(
  "01" = "Oral Exam/Charting",
  "02" = "Radiographic Positioning",
  "03" = "Radiographic Interpretation",
  "04" = "Scaling",
  "05" = "Polishing",
  "06" = "Canine Closed Extraction",
  "07" = "Feline Closed Extraction",
  "08" = "Canine Open Extraction - Single Root",
  "09" = "Feline Open Extraction - Single Root",
  "10" = "Canine Open Extraction - Canines/Multiple Roots",
  "11" = "Feline Open Extraction - Canines/Multiple Roots",
  "12" = "Fluoride Treatment",
  "13" = "None of These"
)

# Respondent counts
n_employers <- nrow(Employer_Data_Clean)
n_educators <- nrow(Educator_Data_Clean)

# Valid variable names (exclude Q20_14 or Q20_NA)
valid_options <- sprintf("%02d", 1:13)
educator_vars <- paste0("Q20_", valid_options)
employer_vars <- paste0("Q25_", valid_options)

# Function to compute percent selected
get_counts <- function(data, vars, n_total) {
  data %>%
    select(all_of(vars)) %>%
    summarise(across(everything(), ~ sum(. == 1, na.rm = TRUE))) %>%
    pivot_longer(cols = everything(), names_to = "option", values_to = "count") %>%
    mutate(
      percent = count / n_total * 100,
      option = str_replace(option, "Q[0-9]+_", "")
    )
}

# Get percentages for each group
educator_percent <- get_counts(Educator_Data_Clean, educator_vars, n_educators) %>%
  mutate(Group = "Educators")

employer_percent <- get_counts(Employer_Data_Clean, employer_vars, n_employers) %>%
  mutate(Group = "Employers")

# Combine and label
combined_percent <- bind_rows(educator_percent, employer_percent) %>%
  filter(option %in% names(option_labels)) %>%
  mutate(
    Label = factor(option_labels[option], levels = rev(option_labels)),
    Group = factor(Group, levels = c("Educators", "Employers"))
  )

# Dot plot
ggplot(combined_percent, aes(x = percent, y = Label, color = Group)) +
  geom_point(aes(size = percent), alpha = 0.8) +
  geom_text(aes(label = paste0(round(percent, 1), "%")), hjust = -0.2, color = "black", fontface = "bold") +
  scale_color_manual(values = c("Educators" = "#1f78b4","Employers" = "#e31a1c"))+
  scale_size(range = c(3, 8), guide = "none") +
  labs(
    x = "% of respondents in respective groups",
    y = NULL,
    title = "Clinical Training Skills Perceived by Educators vs Employers",
    subtitle = "Each dot shows percent of Eduactors or Employers who selected each skill",
    color = NULL
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "top",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold"),
    axis.text.y = element_text(size = 10),
    plot.title.position = "plot"
  ) +
  xlim(0, max(combined_percent$percent, na.rm = TRUE) + 10)

```



```{r echo = FALSE}
# Define labels for each of the 13 skills
option_labels <- c(
  "01" = "Oral Exam/Charting",
  "02" = "Radiographic Positioning",
  "03" = "Radiographic Interpretation",
  "04" = "Scaling",
  "05" = "Polishing",
  "06" = "Canine Closed Extraction",
  "07" = "Feline Closed Extraction",
  "08" = "Canine Open Extraction - Single Root",
  "09" = "Feline Open Extraction - Single Root",
  "10" = "Canine Open Extraction - Canines/Multiple Roots",
  "11" = "Feline Open Extraction - Canines/Multiple Roots",
  "12" = "Fluoride Treatment",
  "13" = "None of These"
)

# Total respondents per group
n_employers <- nrow(Employer_Data_Clean)
n_educators <- nrow(Educator_Data_Clean)

# Generic function to summarize counts from 1s
get_counts <- function(data, vars, n_total) {
  data %>%
    select(all_of(vars)) %>%
    summarise(across(everything(), ~ sum(. == 1, na.rm = TRUE))) %>%
    pivot_longer(everything(), names_to = "option", values_to = "count") %>%
    mutate(
      percent = count / n_total * 100,
      option = str_replace(option, "Q[0-9]+_", "")  # Remove question prefix
    )
}

# Get counts for employers (Q25_01 to Q25_13)
employer_counts <- get_counts(Employer_Data_Clean, paste0("Q25_", str_pad(1:13, 2, pad = "0")), n_employers)

# Get counts for educators (Q20_01 to Q20_13)
educator_counts <- get_counts(Educator_Data_Clean, paste0("Q20_", str_pad(1:13, 2, pad = "0")), n_educators)

# Merge the two groups by skill option
combined_counts <- educator_counts %>%
  rename(educator_count = count, educator_percent = percent) %>%
  inner_join(
    employer_counts %>% rename(employer_count = count, employer_percent = percent),
    by = "option"
  )

# Add skill labels
combined_counts <- combined_counts %>%
  mutate(Skill = option_labels[option])

# Run Fisher‚Äôs Exact Test for each skill
combined_counts <- combined_counts %>%
  rowwise() %>%
  mutate(
    p_value = {
      a <- educator_count
      b <- n_educators - a
      c <- employer_count
      d <- n_employers - c
      fisher.test(matrix(c(a, b, c, d), nrow = 2, byrow = TRUE))$p.value
    }
  ) %>%
  ungroup()

# Format the final table
final_table <- combined_counts %>%
  select(Skill, educator_percent, employer_percent, p_value) %>%
  mutate(
    educator_percent = round(educator_percent, 1),
    employer_percent = round(employer_percent, 1),
    p_value = signif(p_value, 3)
  ) %>%
  rename(
    `% of Educators` = educator_percent,
    `% of Employers` = employer_percent,
    `P-value` = p_value
  ) %>%
  arrange(match(Skill, option_labels))

# Print a clean table
knitr::kable(final_table, col.names = c("Skill", "% of Educators", "% of Employers", "P-value"))
```


### S8 Do educators and employers differ in their opinions about which clinical dentistry skills DVM students should be required to practice or learn during their clinical training?

In question #26 of the employers version of the survey, participants were asked, "Which of the following skills do you think that DVM students should be required to practice/learn as part of the clinical training portion of a DVM program? Select one response for each of the skills listed below." The analogue of this question for educators was survey question #21

This question hits at the sentiment on what both groups think should be required practice/learnings for students. The research question asked if there was a difference in opinions on this matter. To infer from the data, we will use the Mann-Whitney U-Test, a non-parametric test also known as Wilcoxin Rank Sign Test. This test assumes mutual exclusivity between groups.

```{r S8, echo=FALSE, warning=FALSE, fig.align='center'}
educator_cols <- paste0("Q21_", sprintf("%02d", 1:14))
employer_cols  <- paste0("Q26_", sprintf("%02d", 1:14))

# Initialize a results list
mw_results <- list()

# For loop that 
for (i in 1:14) {
  q_edu <- educator_cols[i]
  q_emp <- employer_cols[i]
  
  # Coerce to numeric
  edu_vals <- as.numeric(Educator_Data_Clean[[q_edu]])
  emp_vals <- as.numeric(Employer_Data_Clean[[q_emp]])
  
  # Remove NAs
  edu_vals <- edu_vals[!is.na(edu_vals)]
  emp_vals <- emp_vals[!is.na(emp_vals)]
  
  # Only run test if both groups have >1 value
  if (length(edu_vals) > 1 && length(emp_vals) > 1) {
    test_result <- wilcox.test(edu_vals, emp_vals, exact = FALSE)
    
    mw_results[[q_edu]] <- list(
      question = q_edu,
      W = test_result$statistic,
      p_value = test_result$p.value
    )
  } else {
    mw_results[[q_edu]] <- list(
      question = q_edu,
      W = NA,
      p_value = NA
    )
  }
}


mw_df <- do.call(rbind, lapply(mw_results, as.data.frame))
mw_df <- as.data.frame(mw_df)
mw_df$p_value <- as.numeric(mw_df$p_value)
mw_df$question <- rownames(mw_df)
mw_df$significance <- cut(mw_df$p_value,
                          breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
                          labels = c("***", "**", "*", ".", "ns"))
print(mw_df)
```

```{r S8v2, fig.align='center'}
view(Educator_Data_Clean %>% select(starts_with("Q17")))
view(Employer_Data_Clean %>% select(starts_with("Q21")))
```

## Findings

## Statistical Analysis

# Discussion/Conclusion

## Interpretation of Results

## Implications of the Study

## Limitations

## Recommendations

## Summary of Key Findings

## Final Thoughts

# Appendix
